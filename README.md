# Literature Review Copilot - Implementation Progress
## Using RAG-Anything (LightRAG + MinerU)

## ‚úÖ COMPLETED (MVP with RAG-Anything)

### 1Ô∏è‚É£ Environment Setup ‚úÖ
- ‚úÖ Python structure with RAG-Anything integration
- ‚úÖ LLM backend support (Claude, OpenAI, Ollama) configured for RAG-Anything
- ‚úÖ RAG-Anything repository integrated (using pip package)
- ‚úÖ Vector DB: Using LightRAG's built-in storage (Nano-GraphRAG)
- ‚ö†Ô∏è **TODO**: Git initialization, VS Code extensions

### 2Ô∏è‚É£ Document Ingestion ‚úÖ
- ‚úÖ **RAG-Anything multimodal parsing** (PDF, DOCX, PPTX, images)
- ‚úÖ **MinerU integration** for high-fidelity document extraction
- ‚úÖ Automatic structure preservation (text, images, tables, equations)
- ‚úÖ Multiple file format support out of the box
- ‚ö†Ô∏è **TODO**: TeX conversion (use Pandoc separately), website scraping
- ‚ö†Ô∏è **TODO**: Metadata normalization in YAML frontmatter

### 3Ô∏è‚É£ Semantic Indexing & RAG Setup ‚úÖ
- ‚úÖ **LightRAG** graph-based knowledge representation
- ‚úÖ Dual-graph construction (cross-modal + textual semantics)
- ‚úÖ Multiple query modes: hybrid, local, global, naive
- ‚úÖ Multimodal embedding (text + vision)
- ‚úÖ Cross-modal relationship discovery
- ‚úÖ Semantic retrieval fully functional

### 4Ô∏è‚É£ Comment & Annotation System ‚úÖ
- ‚úÖ HTML-style comments: `<!-- HUMAN_COMMENT -->` and `<!-- AI_COMMENT -->`
- ‚úÖ JSON metadata in comments (type, target_section, timestamp, line_number)
- ‚úÖ Parse and filter comments by type
- ‚úÖ AI cannot overwrite HUMAN_COMMENT
- ‚ö†Ô∏è **TODO**: Git versioning integration
- ‚ö†Ô∏è **TODO**: Character offset linking for precise targeting

### 5Ô∏è‚É£ Concept Lens Layer ‚ö†Ô∏è
- ‚ö†Ô∏è **TODO**: Extract concepts using RAG-Anything's entity extraction
- ‚ö†Ô∏è **TODO**: Leverage LightRAG's knowledge graph for concept relationships
- ‚ö†Ô∏è **TODO**: Compute activation scores on documents
- ‚ö†Ô∏è **TODO**: Generate concept-based AI_COMMENT suggestions

### 6Ô∏è‚É£ AI-Generated Comment System ‚úÖ
- ‚úÖ Pipeline for AI_COMMENT generation using RAG-Anything
- ‚úÖ RAG context retrieval with multimodal support
- ‚úÖ Vision-enhanced queries (VLM) for analyzing figures/charts
- ‚úÖ Append-only insertion into Markdown
- ‚ö†Ô∏è **TODO**: Manual review/accept-reject workflow UI

### 7Ô∏è‚É£ ORKG Integration ‚ùå
- ‚ùå **TODO**: Map documents to ORKG via DOI/title
- ‚ùå **TODO**: Query ORKG API for related papers
- ‚ùå **TODO**: Feed ORKG suggestions into RAG context

### 8Ô∏è‚É£ VS Code Extension Features ‚ùå
- ‚ùå **TODO**: File watcher for Markdown updates
- ‚ùå **TODO**: Command palette actions
- ‚ùå **TODO**: Sidebar panel for comments & concepts
- ‚ùå **TODO**: Accept/edit/delete UI

### 9Ô∏è‚É£ Linking Comments to Chunks ‚ö†Ô∏è
- ‚úÖ Target section linking
- ‚ö†Ô∏è **TODO**: Line number tracking
- ‚ö†Ô∏è **TODO**: Leverage RAG-Anything's chunk IDs for precise linking
- ‚ö†Ô∏è **TODO**: Dynamic link updates

### üîü Git Integration ‚ö†Ô∏è
- ‚ö†Ô∏è **TODO**: Initialize Git repository
- ‚ö†Ô∏è **TODO**: Track comment changes
- ‚ö†Ô∏è **TODO**: Auto-commit workflow

### 1Ô∏è‚É£1Ô∏è‚É£ Optional / Future Enhancements ‚ùå
- ‚ùå Multi-layer concept lens using LightRAG graph
- ‚ùå Incremental updates (LightRAG supports this)
- ‚ùå JabRef integration for bibliography management
- ‚ùå Section-level summarization

---

## üéØ NEXT STEPS (Priority Order)

### Phase 1: Test RAG-Anything Integration ‚è≥
1. **Install RAG-Anything** with all dependencies
2. **Test multimodal parsing** with sample papers (PDFs with figures/tables)
3. **Verify query modes** (hybrid, local, global, naive)
4. **Test vision-enhanced queries** for figure analysis
5. **Configure MinerU** for optimal document extraction

### Phase 2: Enhanced Metadata & Structure üîú
1. Create Markdown export from RAG-Anything results
2. Add YAML frontmatter to exported Markdown
3. Extract DOI, authors, title from parsed documents
4. Link RAG-Anything chunks to Markdown sections

### Phase 3: Git Integration üîú
1. Initialize Git in documents folder
2. Track ingested documents
3. Version control for AI comments
4. `.gitignore` for RAG storage

### Phase 4: Concept Lens with LightRAG Graph üîú
1. **Extract entities** from LightRAG's knowledge graph
2. **Map concepts** across documents using graph relationships
3. **Compute concept importance** based on graph centrality
4. **Generate concept-based suggestions** for new documents

### Phase 5: VS Code Extension üìö
1. Learn TypeScript basics
2. Create extension skeleton
3. Connect to RAG-Anything via Python backend API
4. Implement comment UI

### Phase 6: ORKG Integration üîú
1. ORKG API client
2. DOI-based paper matching
3. Related paper suggestions
4. Integrate with RAG-Anything context

---

## üì¶ DEPENDENCIES & INSTALLATION

### Core Dependencies (RAG-Anything)
```bash
# Full installation with all parsers
pip install raganything[all]

# Core RAG system
pip install lightrag-hku

# LLM providers (choose based on your preference)
pip install anthropic  # For Claude
pip install openai     # For OpenAI/GPT

# Optional: For better document parsing
pip install docling    # Alternative to MinerU
```

### MinerU Setup (Recommended for Best Quality)
```bash
# Install MinerU for high-fidelity PDF extraction
pip install magic-pdf[full]

# Verify installation
mineru --version

# Download models (automatic on first use)
python -c "from raganything import RAGAnything; rag = RAGAnything(); rag.check_mineru_installation()"
```

### Environment Variables
```bash
# Choose LLM provider
export LLM_PROVIDER="claude"  # or "openai" or "ollama"

# API Keys
export ANTHROPIC_API_KEY="your-claude-key"
export OPENAI_API_KEY="your-openai-key"

# Optional: Ollama
export OLLAMA_BASE_URL="http://localhost:11434/v1"
```

### Future Dependencies
```bash
# Phase 3: Git
pip install gitpython

# Phase 5: VS Code Extension
npm install -g yo generator-code

# Phase 6: ORKG
pip install requests
```

---

## üìù USAGE EXAMPLES

### Current MVP Commands

```bash
# 1. Ingest a document (PDF, DOCX, PPTX, images)
python copilot.py ingest paper.pdf

# RAG-Anything will:
# - Parse with MinerU (extracts text, images, tables, equations)
# - Create multimodal embeddings
# - Build knowledge graph
# - Store in LightRAG format

# 2. Search with different modes
python copilot.py search "transformer architecture" hybrid
python copilot.py search "self-attention mechanism" local
python copilot.py search "what are the main contributions" global

# 3. Process Markdown and add AI comments
python copilot.py process documents/paper.md
```

### Query Modes Explained
- **hybrid**: Combines graph navigation + semantic search (best for most queries)
- **local**: Focuses on nearby entities in knowledge graph
- **global**: Broad semantic search across all content
- **naive**: Simple text matching (fastest but less intelligent)

### File Structure
```
.
‚îú‚îÄ‚îÄ copilot.py              # Main MVP script with RAG-Anything
‚îú‚îÄ‚îÄ documents/              # Markdown files (will be created)
‚îú‚îÄ‚îÄ imports/                # Original PDFs/documents
‚îú‚îÄ‚îÄ rag_storage/            # LightRAG knowledge graph & embeddings
‚îÇ   ‚îú‚îÄ‚îÄ graph_chunk_entity_relation.graphml
‚îÇ   ‚îú‚îÄ‚îÄ kv_store_*.json
‚îÇ   ‚îî‚îÄ‚îÄ vdb_*.json
‚îî‚îÄ‚îÄ assets/                 # Extracted images/figures
```

---

## üîç WHAT'S WORKING NOW

‚úÖ **RAG-Anything Features Available:**
- **Multimodal document parsing** (text, images, tables, equations)
- **High-fidelity extraction** with MinerU
- **Graph-based knowledge representation** (LightRAG)
- **Cross-modal retrieval** (find related content across different modalities)
- **Multiple query modes** for different use cases
- **Vision-enhanced queries** (analyze charts/figures)
- **Automatic entity & relationship extraction**

‚úÖ **Your Custom Features:**
- Comment system (HUMAN_COMMENT, AI_COMMENT)
- AI comment generation with RAG context
- Multi-LLM support (Claude, OpenAI, Ollama)
- Structured comment metadata

---

## üöß KEY DIFFERENCES FROM CUSTOM RAG

### What RAG-Anything Provides (vs custom implementation):
1. **Better Document Parsing**: MinerU extracts formulas, tables, charts with high fidelity
2. **Multimodal Support**: Handles images, not just text
3. **Graph-Based RAG**: LightRAG uses knowledge graphs vs simple vector search
4. **Entity Relationships**: Automatically discovers connections between concepts
5. **Vision Models**: Can analyze figures and charts in papers
6. **Production Ready**: Battle-tested, maintained by HKU research team

### What You Still Need to Build:
1. Comment system (HTML comments in Markdown) ‚úÖ Done
2. Git integration
3. VS Code extension
4. ORKG integration
5. Concept lens visualization
6. Accept/reject workflow

---

## üí° RECOMMENDATIONS

### Immediate (Today)
1. **Install RAG-Anything**: `pip install raganything[all] lightrag-hku anthropic`
2. **Set environment variables** for your LLM
3. **Test with 1-2 papers** to verify multimodal parsing works

### This Week
1. **Ingest 5-10 papers** to build a knowledge base
2. **Test different query modes** to understand their behavior
3. **Try vision-enhanced queries** on papers with figures
4. **Document metadata extraction** workflow

### Next 2 Weeks
1. **Markdown export** from RAG-Anything results
2. **Add Git integration** for version control
3. **Refine AI comment generation** using LightRAG context

### Next Month
1. **Concept lens** using LightRAG's knowledge graph
2. **VS Code extension** basic prototype
3. **ORKG integration** for bibliography enrichment

---

## üéì LEARNING RESOURCES

### RAG-Anything & LightRAG
- [RAG-Anything GitHub](https://github.com/HKUDS/RAG-Anything)
- [RAG-Anything Paper (arXiv)](https://arxiv.org/abs/2510.12323)
- [LightRAG Documentation](https://github.com/HKUDS/LightRAG)
- [MinerU Documentation](https://github.com/opendatalab/MinerU)

### Multimodal RAG
- LightRAG architecture explanation
- Cross-modal retrieval techniques
- Vision-Language Models (VLM) integration

### VS Code Extension Development
- [Official Guide](https://code.visualstudio.com/api/get-started/your-first-extension)
- TypeScript basics

---

## üî• ADVANTAGES OF THIS APPROACH

1. **Production-Ready RAG**: Don't reinvent the wheel - RAG-Anything is research-grade
2. **Multimodal Native**: Perfect for research papers with figures/tables/equations
3. **Graph-Based**: Better understanding of concept relationships
4. **Active Development**: Maintained by top research lab
5. **Flexible**: Works with Claude, OpenAI, or Ollama
6. **Extensible**: Easy to add custom layers (comments, ORKG, etc.)

---

## üìä TESTING CHECKLIST

Before Phase 2:

- [ ] RAG-Anything installed and initialized
- [ ] Can ingest PDFs with figures/tables
- [ ] Multiple LLM providers work (Claude/OpenAI/Ollama)
- [ ] Hybrid query mode returns relevant results
- [ ] Vision-enhanced queries work on figures
- [ ] Knowledge graph is built correctly
- [ ] AI comments are generated with good context
- [ ] HUMAN_COMMENT vs AI_COMMENT are properly separated
- [ ] Comment metadata is well-structured

---

## ü§ù NEXT COLLABORATION POINTS

Once RAG-Anything is working:

1. **JabRef Integration** (if desired)
   - Export from RAG-Anything to BibTeX
   - Sync bibliography with JabRef database
   - Link PDF annotations

2. **Advanced Features**
   - Concept activation visualization
   - Multi-document synthesis
   - Automatic literature review generation

3. **UI Development**
   - Web interface for document viewing
   - VS Code extension for inline comments
   - Graph visualization for concept relationships
